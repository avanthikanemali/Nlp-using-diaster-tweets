# -*- coding: utf-8 -*-
"""diastertweets task3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jy2hOsKz3HyOoAfyXdMRBhsn4LtKabPl
"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import RandomizedSearchCV
import joblib
import string
import numpy as np

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Load the CSV file
file_path = '/content/Diastertweets_traincleaned.csv'  # Use the path to your uploaded CSV file
df = pd.read_csv(file_path)

# Preprocessing function to clean the text
def preprocess_text(text):
    text = str(text).lower()  # Convert to lowercase and ensure the text is a string
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text

# Apply preprocessing to the text column
df['cleaned_text'] = df['text'].apply(preprocess_text)

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Increase to top 5000 features

# Fit and transform the cleaned text
tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])

# Convert the TF-IDF matrix to a DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Split the data into training and testing sets
X = tfidf_df
y = df['target']  # Assuming 'target' is the column with labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV with n_jobs=-1 for parallel processing
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=20, cv=3, n_jobs=-1, verbose=2, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Get the best parameters
best_params = random_search.best_params_
print("Best Parameters:", best_params)

# Train the Random Forest model with best parameters
rf_best = RandomForestClassifier(**best_params)
rf_best.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_best.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Save the TF-IDF vectorizer
joblib.dump(tfidf_vectorizer, '/content/tfidf_vectorizer.pkl')

# Save the trained Random Forest model
joblib.dump(rf_best, '/content/best_rfc_model.pkl')

# Function to clean text for new predictions
def clean_text(text):
    text = text.lower()
    text = re.sub(r'@\S+', '', text)  # Remove twitter handles
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'pic.\S+', '', text)
    text = re.sub(r"[^a-zA-Z+']", ' ', text)  # Keep only characters
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text + ' ')  # Keep words with length > 1
    text = "".join([i for i in text if i not in string.punctuation])
    words = nltk.tokenize.word_tokenize(text)
    stopwords_list = nltk.corpus.stopwords.words('english')
    text = " ".join([i for i in words if i not in stopwords_list and len(i) > 2])
    text = re.sub("\s[\s]+", " ", text).strip()
    return text

# Function to find the optimal threshold
def find_optimal_threshold(model, X_val, y_val):
    thresholds = np.arange(0.0, 1.0, 0.01)
    best_threshold = 0.5
    best_f1 = 0

    for threshold in thresholds:
        y_val_pred_prob = model.predict_proba(X_val)[:, 1]
        y_val_pred = (y_val_pred_prob >= threshold).astype(int)
        f1 = f1_score(y_val, y_val_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    return best_threshold

# Split the test set further to find the optimal threshold
X_val, X_test_final, y_val, y_test_final = train_test_split(X_test, y_test, test_size=0.5, random_state=42)
optimal_threshold = find_optimal_threshold(rf_best, X_val, y_val)
print(f"Optimal Threshold: {optimal_threshold}")

# Function to predict new tweets
def predict_new_tweet(new_tweet, threshold=0.5):
    # Clean the new tweet
    cleaned_tweet = clean_text(new_tweet)

    # Transform the cleaned tweet using the pre-trained TF-IDF vectorizer
    tweet_tfidf = tfidf_vectorizer.transform([cleaned_tweet])

    # Predict the probability of the tweet being a disaster
    tweet_score = rf_best.predict_proba(tweet_tfidf)[:, 1][0]

    # Classify the tweet based on the optimal threshold
    prediction = 1 if tweet_score >= threshold else 0

    return prediction, tweet_score

# Example usage
new_tweet = input("Enter the tweet: ")
prediction, score = predict_new_tweet(new_tweet, optimal_threshold)

print(f"Prediction: {'Disaster' if prediction == 1 else 'Not a Disaster'}")
print(f"Score: {score}")

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.model_selection import RandomizedSearchCV
import joblib
import string
import numpy as np

# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')

# Preprocessing function to clean the text
def preprocess_text(text):
    text = str(text).lower()  # Convert to lowercase and ensure the text is a string
    text = re.sub(r'\d+', '', text)  # Remove numbers
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
    return text

# Apply preprocessing to the text column
df['cleaned_text'] = df['text'].apply(preprocess_text)

# Initialize TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # Increase to top 5000 features

# Fit and transform the cleaned text
tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text'])

# Convert the TF-IDF matrix to a DataFrame
tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

print(tfidf_df.head())

# Split the data into training and testing sets
X = tfidf_df
y = df['target']  # Assuming 'target' is the column with labels
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the parameter grid for RandomizedSearchCV
param_dist = {
    'n_estimators': [100, 200, 300, 400, 500],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Initialize RandomizedSearchCV with n_jobs=-1 for parallel processing
random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=20, cv=3, n_jobs=-1, verbose=2, random_state=42)

# Fit RandomizedSearchCV
random_search.fit(X_train, y_train)

# Get the best parameters
best_params = random_search.best_params_
print("Best Parameters:", best_params)

# Train the Random Forest model with best parameters
rf_best = RandomForestClassifier(**best_params)
rf_best.fit(X_train, y_train)

# Predict on the test set
y_pred = rf_best.predict(X_test)

# Calculate metrics
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# Save the TF-IDF vectorizer
joblib.dump(tfidf_vectorizer, '/content/tfidf_vectorizer.pkl')

# Save the trained Random Forest model
joblib.dump(rf_best, '/content/best_rfc_model.pkl')

#Function to clean text for new predictions
def clean_text(text):
    text = text.lower()
    text = re.sub(r'@\S+', '', text)  # Remove twitter handles
    text = re.sub(r'http\S+', '', text)  # Remove URLs
    text = re.sub(r'pic.\S+', '', text)
    text = re.sub(r"[^a-zA-Z+']", ' ', text)  # Keep only characters
    text = re.sub(r'\s+[a-zA-Z]\s+', ' ', text + ' ')  # Keep words with length > 1
    text = "".join([i for i in text if i not in string.punctuation])
    words = nltk.tokenize.word_tokenize(text)
    stopwords_list = nltk.corpus.stopwords.words('english')
    text = " ".join([i for i in words if i not in stopwords_list and len(i) > 2])
    text = re.sub("\s[\s]+", " ", text).strip()
    return text

# Function to find the optimal threshold
def find_optimal_threshold(model, X_val, y_val):
    thresholds = np.arange(0.0, 1.0, 0.01)
    best_threshold = 0.5
    best_f1 = 0

    for threshold in thresholds:
        y_val_pred_prob = model.predict_proba(X_val)[:, 1]
        y_val_pred = (y_val_pred_prob >= threshold).astype(int)
        f1 = f1_score(y_val, y_val_pred)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = threshold
    return best_threshold

# Split the test set further to find the optimal threshold
X_val, X_test_final, y_val, y_test_final = train_test_split(X_test, y_test, test_size=0.5, random_state=42)
optimal_threshold = find_optimal_threshold(rf_best, X_val, y_val)
print(f"Optimal Threshold: {optimal_threshold}")

# Function to predict new tweets
def predict_new_tweet(new_tweet, threshold=0.5):
    # Clean the new tweet
    cleaned_tweet = clean_text(new_tweet)

    # Transform the cleaned tweet using the pre-trained TF-IDF vectorizer
    tweet_tfidf = tfidf_vectorizer.transform([cleaned_tweet])

    # Predict the probability of the tweet being a disaster
    tweet_score = rf_best.predict_proba(tweet_tfidf)[:, 1][0]

    # Classify the tweet based on the optimal threshold
    prediction = 1 if tweet_score >= threshold else 0

    return prediction, tweet_score

# Example usage
new_tweet = input("Enter the tweet: ")
prediction, score = predict_new_tweet(new_tweet, optimal_threshold)

print(f"Prediction: {'Disaster' if prediction == 1 else 'Not a Disaster'}")
print(f"Score: {score}")