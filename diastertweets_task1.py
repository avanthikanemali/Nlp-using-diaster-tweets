# -*- coding: utf-8 -*-
"""Diastertweets task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pjG4uRUdQk8szi8-GeWw7XHEVlwHRUg_
"""

import pandas as pd

# Read the original CSV file
original_csv_path = '/content/tweets.csv'
df = pd.read_csv(original_csv_path)

# Extract the first 1500 rows
df_first_2500 = df.head(2500)

# Save the new DataFrame to a new CSV file
new_csv_path = '/content/tweetsrevised.csv'
df_first_2500.to_csv(new_csv_path, index=False)

print(f"The first 1500 rows have been saved to {new_csv_path}")

import pandas as pd

# Load the dataset
train_df = pd.read_csv('/content/tweets.csv')

# Display the first few rows to understand the structure
print(train_df.head())

# Check for missing or empty locations
print(train_df['location'].isnull().sum())
print((train_df['location'] == '').sum())

# Drop rows where 'location' is NaN or empty
train_df = train_df.dropna(subset=['location'])
train_df = train_df[train_df['location'] != '']

# Verify the rows have been dropped
print(train_df['location'].isnull().sum())
print((train_df['location'] == '').sum())

# Save the cleaned dataset
train_df.to_csv('/content/tweetswloc.csv', index=False)

import pandas as pd
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the dataset
train_df = pd.read_csv('/content/tweetswloc.csv')

# Data Cleaning
train_df = train_df.fillna('')
train_df = train_df.drop_duplicates()

# Normalize Text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\@w+|\#', '', text)
    text = re.sub(r'[^A-Za-z0-9]+', ' ', text)
    return text

train_df['text'] = train_df['text'].apply(clean_text)

# Tokenization and Padding
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(train_df['text'])
word_index = tokenizer.word_index
train_sequences = tokenizer.texts_to_sequences(train_df['text'])
train_padded = pad_sequences(train_sequences, maxlen=100, padding='post', truncating='post')

# Save the cleaned and consolidated dataset
train_df.to_csv('/content/Diastertweets_train.csv', index=False)

import pandas as pd
import re
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Load the dataset
df = pd.read_csv('/content/Diastertweets_train.csv')

# Handle missing values
df = df.dropna(subset=['location'])
df['text'] = df['text'].fillna('')

# Remove duplicates
df = df.drop_duplicates()

# Normalize text
def clean_text(text):
    text = text.lower()
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\@\w+|\#', '', text)
    text = re.sub(r'[^A-Za-z0-9\s]+', '', text)
    text = text.strip()
    return text

df['text'] = df['text'].apply(clean_text)

# Drop the 'id' column
df = df.drop(columns=['id'])

# Tokenization and padding (if necessary)
tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')
tokenizer.fit_on_texts(df['text'])
word_index = tokenizer.word_index

sequences = tokenizer.texts_to_sequences(df['text'])
padded_sequences = pad_sequences(sequences, maxlen=100, padding='post', truncating='post')

# Save the cleaned dataset
df.to_csv('/content/Diastertweets_traincleaned.csv', index=False)